{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        If parent_properties provided, merge them with each page's properties.\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties with specific handling for Name, Description, and Tags.\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract properties from a page.\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content by:\n",
    "        1. Replacing multiple spaces with single space\n",
    "        2. Removing spaces before newlines\n",
    "        3. Removing spaces after newlines\n",
    "        4. Removing empty lines\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"Extract text content from rich text array and normalize it.\"\"\"\n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"Retrieve all child blocks of a given block with their nesting level.\"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content.\n",
    "        Sub-headers are treated as text content with line breaks.\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"Merge a group of bullets into a single line, with sub-bullets inline.\"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Process a single page and its nested databases.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"Process entire database and return structured data.\"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "COLLECTION_NAME = \"Notion_vector_store\"\n",
    "\n",
    "model_kwargs = {\"model\": \"gpt-4o-mini\", \n",
    "                \"temperature\": 0,\n",
    "                \"api_key\": OPENAI_API_KEY,\n",
    "                \"system_prompt\": \"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "                Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "                \"\"\"                \n",
    "                }\n",
    "\n",
    "llm_openai = OpenAI(**model_kwargs)\n",
    "llm_cohere = Cohere(model=\"command-r-plus\", api_key=CO_API_KEY)\n",
    "\n",
    "embed_model_openai = OpenAIEmbedding(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "embed_model_cohere = CohereEmbedding(model=\"embed-english-v3.0\", api_key=CO_API_KEY)\n",
    "\n",
    "# set up the vector store\n",
    "client = QdrantClient(location=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "aclient = AsyncQdrantClient(location=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, aclient=aclient, collection_name=COLLECTION_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the database\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.readers.notion import NotionPageReader\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID)\n",
    "\n",
    "documents = [Document(text=record['content'], metadata=record['properties']) for record in processed_data]\n",
    "# documents = NotionPageReader(integration_token=NOTION_TOKEN).load_data(\n",
    "#     database_ids=[DATABASE_ID]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest to Vector Database\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512, # in tokens\n",
    "    chunk_overlap=16, # in tokens\n",
    "    paragraph_separator=\"\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "tranforms = [\n",
    "    sentence_splitter, \n",
    "    embed_model_openai\n",
    "    ]\n",
    "\n",
    "nodes = IngestionPipeline(\n",
    "    documents=documents,\n",
    "    transformations=tranforms,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00, 11.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build index over vector database\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "query_engine_kwargs = {\n",
    "    \"llm\": llm_openai,\n",
    "    \"embed_model\": embed_model_openai,\n",
    "    \"response_mode\": \"compact\",\n",
    "    \"similarity_top_k\": 15,\n",
    "    \"vector_store_query_mode\": \"mmr\",\n",
    "    \"vector_store_kwargs\": {\"mmr_threshold\": 0.42}\n",
    "    }\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    "    )\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    embed_model=embed_model_openai,\n",
    "    vector_store=vector_store,\n",
    "    )\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents=documents,\n",
    "#     embed_model=embed_model_openai,\n",
    "#     vector_store=vector_store\n",
    "#     )\n",
    "\n",
    "# index = VectorStoreIndex(\n",
    "#     documents,\n",
    "#     show_progress=True,\n",
    "#     store_nodes_override=True,\n",
    "#     transformation=[sentence_splitter],\n",
    "#     embed_model=embed_model_openai,\n",
    "#     storage_context=storage_context,\n",
    "# )\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm_openai)\n",
    "#query_engine = index.as_chat_engine()\n",
    "#query_engine = index.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='f932adb4-0fd0-4fcb-9030-a55d8c4eae45', embedding=None, metadata={'page_id': 'f932adb4-0fd0-4fcb-9030-a55d8c4eae45'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\nProject Overview:\\nThe Automated Sports Betting Tracker and Data Extractor Project involves the development of a set of Python scripts to automate the extraction and processing of betting details from HTML files provided by two bookmakers, Bet365 and Fanduel. The project is designed to parse HTML content, extract relevant betting information, process the data, and save it into a structured CSV format for further analysis or reporting. See the GitHub for full details on the project.\\nTasks Performed:\\nInitial Setup and Configuration:\\n\\tCreated a configuration script (config.py) to define data models, classification templates, and helper functions.\\n\\tSet up an environment for using OpenAI's API for further processing and classification of betting data.\\nDevelopment of Abstract Base Class and Specific Implementations:\\n\\tDesigned an abstract base class to standardize the extraction and processing methods.\\n\\tImplemented specific extractor classes (Bet365 and Fanduel) to handle the unique HTML structures and data requirements of each bookmaker.\\nHTML Parsing and Data Extraction:\\n\\tUtilized BeautifulSoup to parse HTML content from files and extract bet summaries.\\n\\tImplemented extraction logic in Bet365 and Fanduel classes to gather details such as bet type, selections, odds, stake, return, and bonus bet information.\\nData Processing and Formatting:\\n\\tProcessed the extracted details to format the data appropriately and determine bet statuses (win, loss, pending).\\n\\tUsed a language model to classify and further process the extracted data based on predefined categories.\\nOutput to CSV:\\n\\tSaved the processed data into CSV files with a consistent column order for easy analysis and reporting.\\nChallenges and Solutions:\\nChoosing an Appropriate Use for Utilizing the LLM:\\n\\tChallenge\\n\\t: Initially, the raw HTML code for each bet was fed directly into the LLM. The LLM would only occasionally output the relevant information in the desired structured format.\\n\\tSolution\\n\\t: It was more effective to process the HTML code using a deterministic method first, and use the LLM to supplement information that was lacking,\\nTechnologies Used:\\nPython\\n: The core programming language used for scripting and automation.\\nBeautifulSoup\\n: A library for parsing HTML and extracting data.\\nPydantic\\n: Used for data validation and settings management.\\nCSV\\n: For reading from and writing to CSV files.\\nOpenAI API\\n: Utilized for advanced language processing and classification.\\nAbstract Base Class (ABC)\\n: Used to define standard methods and properties for data extraction classes.\\nLangChain: \\nFor orchestrating and managing the language model interactions\\nSkills Developed:\\nHTML Parsing and Data Extraction\\n: Gained expertise in using BeautifulSoup to navigate and extract data from complex HTML structures.\\nObject-Oriented Programming (OOP):\\n Enhanced skills in designing and implementing abstract base classes and their specific implementations.\\nData Processing and Formatting\\n: Improved ability to process and format extracted data for structured output.\\nAPI Integration\\n: Developed skills in integrating and utilizing external APIs (OpenAI) for advanced data processing tasks.\\nFile I/O Operations\\n: Strengthened proficiency in handling file input/output operations, particularly reading from and writing to CSV files.\\nProject Management\\n: Enhanced ability to manage and coordinate multiple components of a project to achieve a cohesive and functional outcome.\\nAccomplishments:\\nAutomated Betting Data Extraction\\n: Developed Python scripts that automate the extraction and processing of betting information from Bet365 and Fanduel HTML files.\\nAccurate Data Processing\\n: Implemented systematic methods to process and classify betting data, ensuring accuracy and consistency.\\nEffective Use of LLM\\n: Utilized OpenAI's language model to classify and structure betting information, enhancing reliability.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Pipeline\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = QueryPipeline(\n",
    "    chain=chain,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 744b5471-615e-4289-8d76-ef4d373713d5 with input: \n",
      "input: Name projects and their associated challenges\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 074ee98f-ad24-4823-bcba-8297f44081e5 with input: \n",
      "input: Name projects and their associated challenges\n",
      "\n",
      "\u001b[0mThe projects and their associated challenges are as follows:\n",
      "\n",
      "1. **Project on Software Unit Tests Generation**:\n",
      "   - **Challenges**:\n",
      "     - **GPU Cluster Issues**: Faced difficulties in distributing the model across several GPUs, which was attributed to a driver incompatibility problem. The solution involved using a single GPU at reduced precision.\n",
      "     - **Poor Data Quality**: Encountered discrepancies in the dataset, such as mismatched variable names and commands. This was addressed by flagging issues, selecting high-quality examples, and producing documentation with recommendations for fixing them.\n",
      "     - **Reproducibility**: Ensured near-identical results by setting a seed for consistent \"randomness.\"\n",
      "\n",
      "2. **Quadcopter Development Project**:\n",
      "   - **Challenges**:\n",
      "     - **Component Integration**: Ensured reliable communication between sensors, controllers, and cameras, which required thorough testing and high-quality connections.\n",
      "     - **Flight Stability**: Addressed issues like motor twitching and calibration errors by analyzing flight logs and performing comprehensive calibrations.\n",
      "     - **Autonomous Flight Implementation**: Faced complexity and safety concerns with computer vision-based autonomous flight, which required iterative development and the integration of robust safety mechanisms.\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "\n",
    "query = \"Name projects and their associated challenges\"\n",
    "\n",
    "response = query_pipeline.run(input=query)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
