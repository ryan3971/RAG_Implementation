{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up observability with Llamatrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import (\n",
    "    OTLPSpanExporter as HTTPSpanExporter,\n",
    ")\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "# PHOENIX_API_KEY = \"\"\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "\n",
    "# Add Phoenix\n",
    "span_phoenix_processor = SimpleSpanProcessor(\n",
    "    HTTPSpanExporter(endpoint=\"https://app.phoenix.arize.com/v1/traces\")\n",
    ")\n",
    "\n",
    "# Add them to the tracer\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(span_processor=span_phoenix_processor)\n",
    "\n",
    "# Instrument the application\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up the Notion Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        If parent_properties provided, merge them with each page's properties.\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties with specific handling for Name, Description, and Tags.\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract properties from a page.\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content by:\n",
    "        1. Replacing multiple spaces with single space\n",
    "        2. Removing spaces before newlines\n",
    "        3. Removing spaces after newlines\n",
    "        4. Removing empty lines\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"Extract text content from rich text array and normalize it.\"\"\"\n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"Retrieve all child blocks of a given block with their nesting level.\"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content.\n",
    "        Sub-headers are treated as text content with line breaks.\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"Merge a group of bullets into a single line, with sub-bullets inline.\"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Process a single page and its nested databases.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"Process entire database and return structured data.\"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Custom Retreival\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KeywordTableSimpleRetriever,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableSimpleRetriever,\n",
    "        mode: str = \"AND\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the LLM, Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbt7r\\OneDrive\\Documents\\VSCode Workspace\\RAG_Implementation\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"Notion_vector_store\"\n",
    "\n",
    "# Set up the LLM\n",
    "llm_openai = OpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.5,\n",
    "    api_key=OPENAI_API_KEY,,\n",
    "    )\n",
    "\n",
    "Settings.llm = llm_openai\n",
    "\n",
    "# Set up the Embeddings\n",
    "embed_model_openai = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "Settings.embed_model = embed_model_openai\n",
    "\n",
    "# set up the vector store client\n",
    "client = QdrantClient(\n",
    "    location=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the async client\n",
    "aclient = AsyncQdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the Notion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID)\n",
    "\n",
    "documents = [Document(text=record['content'], metadata=record['properties']) for record in processed_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we need to:\n",
    "- Transform the data (Split large documents into chunks and produce Keywords that are also embedded)\n",
    "- Store in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m transforms \u001b[38;5;241m=\u001b[39m [sentence_splitter, embed_model_openai]\u001b[38;5;66;03m#, keyword_extractor]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Ingest into Database\u001b[39;00m\n\u001b[0;32m     19\u001b[0m nodes \u001b[38;5;241m=\u001b[39m IngestionPipeline(\n\u001b[0;32m     20\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransforms,\n\u001b[0;32m     21\u001b[0m     vector_store\u001b[38;5;241m=\u001b[39mvector_store\n\u001b[1;32m---> 22\u001b[0m     )\u001b[38;5;241m.\u001b[39mrun(nodes\u001b[38;5;241m=\u001b[39m\u001b[43mdocuments\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "# Define the sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "#keyword_extractor = KeywordExtractor(keywords=5, llm=llm_openai)\n",
    "\n",
    "# define the transform\n",
    "transforms = [sentence_splitter, embed_model_openai]#, keyword_extractor]\n",
    "\n",
    "# Ingest into Database\n",
    "nodes = IngestionPipeline(\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index over Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    "    )\n",
    "\n",
    "# Create index from vector store with storage context\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,  # Add storage context\n",
    "    embed_model=embed_model_openai,\n",
    "    show_progress=True  # Optional: helps track progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MetadataAwareTextSplitter\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "def organize_chunks_by_project(nodes):\n",
    "    \"\"\"\n",
    "    Organizes retrieved chunks by project and heading, merging related content.\n",
    "    \"\"\"\n",
    "    # Group nodes by project\n",
    "    project_groups = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for node in nodes:\n",
    "        metadata = node.metadata\n",
    "        project = metadata.get('project', 'Unknown Project')\n",
    "        heading = metadata.get('heading', 'General')\n",
    "        \n",
    "        # Add to appropriate group\n",
    "        project_groups[project][heading].append(node)\n",
    "    \n",
    "    # Merge nodes within each project+heading group\n",
    "    merged_nodes = []\n",
    "    for project, heading_groups in project_groups.items():\n",
    "        for heading, nodes_group in heading_groups.items():\n",
    "            # Combine text from all nodes in group\n",
    "            combined_text = \"\\n\".join([n.text for n in nodes_group])\n",
    "            \n",
    "            # Get project metadata from first node\n",
    "            project_metadata = nodes_group[0].metadata\n",
    "            project_desc = project_metadata.get('project_description', '')\n",
    "            \n",
    "            # Create enriched text with metadata header\n",
    "            enriched_text = f\"\"\"Project: {project}\n",
    "Project Description: {project_desc}\n",
    "Section: {heading}\n",
    "---\n",
    "{combined_text}\"\"\"\n",
    "            \n",
    "            # Create new node with merged content\n",
    "            merged_node = nodes_group[0].copy()\n",
    "            merged_node.text = enriched_text\n",
    "            merged_nodes.append(merged_node)\n",
    "    \n",
    "    return merged_nodes\n",
    "\n",
    "# Create post-retrieval transformation function\n",
    "def post_retrieval_transform(retrieval_output):\n",
    "    nodes = [r.node for r in retrieval_output]\n",
    "    organized_nodes = organize_chunks_by_project(nodes)\n",
    "    \n",
    "    # Reconstruct retrieval output with organized nodes\n",
    "    transformed_output = []\n",
    "    for i, node in enumerate(organized_nodes):\n",
    "        # Preserve original retrieval output structure but with new nodes\n",
    "        new_ret = retrieval_output[0].copy()\n",
    "        new_ret.node = node\n",
    "        new_ret.score = retrieval_output[i].score if i < len(retrieval_output) else 0.0\n",
    "        transformed_output.append(new_ret)\n",
    "    \n",
    "    return transformed_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Agent for Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreInfo, MetadataInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"The transformation of a query into sub-queries\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"query_rewriting\",\n",
    "            type=\"str\",\n",
    "            description=\"Useful for finding the right thinker's words to answer a question.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"query_decomposition\",\n",
    "            type=\"str\",\n",
    "            description=\"Use this when the user asks a question.\"\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AutoRetrieveModel(BaseModel):\n",
    "    query: str = Field(..., description=\"A question the user wants to advice about\")\n",
    "    filter_key_list: List[str] = Field(\n",
    "        ..., description=\"List of metadata filter field names\"\n",
    "    )\n",
    "    filter_value_list: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of metadata filter field values (corresponding to names specified in filter_key_list)\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "description = f\"\"\"\\\n",
    "Use this tool to answer the usery query by retrieving relevant information from the vector database.\n",
    "The vector database schema is given below, which you should use to find the right information to answer the user's query.:\n",
    "{vector_store_info.json()}\n",
    "\"\"\"\n",
    "\n",
    "auto_retrieve_tool = FunctionTool.from_defaults(\n",
    "    fn=auto_retrieve_fn,\n",
    "    name=\"words-of-senpai-info\",\n",
    "    description=description,\n",
    "    fn_schema=AutoRetrieveModel\n",
    ")\n",
    "\n",
    "auto_retrieve_tool = FunctionTool.from_defaults(\n",
    "    fn=auto_retrieve_fn,\n",
    "    name=\"words-of-senpai-info\",\n",
    "    description=description,\n",
    "    fn_schema=AutoRetrieveModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept a query fom the user\n",
    "query = \"Test query\"\n",
    "# Feed query into the LLM with a prompt to analyze the query and assess the decomposition method and number of queries to use\n",
    "\n",
    "# Have it output in a structured format so this can be used in the selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QueryFusionRetriever.__init__() got an unexpected keyword argument 'post_retrieval_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m vector_retriever \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_retriever(similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     12\u001b[0m bm25_retriever \u001b[38;5;241m=\u001b[39m BM25Retriever\u001b[38;5;241m.\u001b[39mfrom_defaults(nodes\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mget_nodes(), similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mQueryFusionRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvector_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25_retriever\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to 1 to disable query generation\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreciprocal_rerank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_openai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_retrieval_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_retrieval_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_gen_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQUERY_GEN_PROMPT_TEMPLATE\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: QueryFusionRetriever.__init__() got an unexpected keyword argument 'post_retrieval_transform'"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from prompts import QUERY_GEN_PROMPT\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE = PromptTemplate(QUERY_GEN_PROMPT)\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=index.vector_store.get_nodes(), similarity_top_k=10)\n",
    "\n",
    "# Define your main query and the specific sub-queries manually\n",
    "main_query = \"Explain the impact of AI on healthcare\"\n",
    "manual_sub_queries = [\n",
    "    \"What are the benefits of AI in healthcare?\",\n",
    "    \"What are the potential risks of AI in healthcare?\",\n",
    "    \"How has AI improved patient outcomes in healthcare?\",\n",
    "    \"What ethical concerns arise with AI in healthcare?\",\n",
    "]\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=10,\n",
    "    \n",
    "    num_queries=3,  # Set to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    llm=llm_openai,\n",
    "    query_gen_prompt=QUERY_GEN_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = retriever.retrieve(\"What challenges were encountered and how were they overcome\")\n",
    "for r in ret:\n",
    "    print(f\"Score: {r.score}\")\n",
    "    print(f\"Text: {r.node.text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.transform_nodes = post_retrieval_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What difficulties arose during the process, and what strategies were used to resolve them?\n",
      "2. What hurdles were faced, and what approaches were taken to address these issues?\n",
      "Score: 0.09475006400409626\n",
      "Text: Data Workflow and Text Formatting: Challenge: Determining the workflow for getting data from the application's text editor to a Word document while maintaining the text format. This involved selecting a Node module for exporting the data from the Quill editor to the Python script and selecting the Python library to handle saving the data to a Word document. Solution: Tested several different modules and Python packages, logging their features and shortcomings, eventually settling on ones that best fit the needs of the project. Non-Intrusive User Reminders Challenge: Satisfying the requirement of reminding the user to log their work while avoiding disrupting them mid-thought Solution: Implemented a fade-in animation so the reminder window gently appears without drawing too much attention. Placed the window in the bottom right corner to keep it out of the way. Ensured it would always be the top window so it was always in view , acting as a subtle reminder for the user to take a note when they finish their current task. Conditional Application Runtime Challenge: Ensuring the application runs only during periods when the user is performing relevant work they want to log. Solution: Implemented a settings feature that allows the user to define their working hours, with advanced settings for greater flexibility in when the application runs day to day. Image Resizing and Indented Lists formatting issues Challenge: Images resized in the editor did not retain their dimensions when exported to the Word document and nested bullet points did not retain their nested format when exported. Solution: Modified the Python library responsible for exporting HTML to Word to handle resized images correctly and so it correctly interpreted and applied indentations for nested lists, ensuring they appear as intended in the Word document. Error Handling and Invalid Inputs Challenge: Preventing invalid user inputs within the settings and handling errors effectively. Solution: Implemented alerts to handle any invalid cases, providing immediate feedback to the user and preventing the application from proceeding with invalid settings. Window Rendering Challenge: Windows were loading before they finished rendering, leading to incomplete displays and a poor user experience.\n",
      "--------------------------------------------------\n",
      "Score: 0.07992634605537831\n",
      "Text: Device Design Challenge: Design a user friendly device that could easily be attached/removed while recording reliable EMG data. Solution: Conducted extensive research on EMG technology and settled on creating a 3D printed armband with integrated electrodes, opting for quantity (of data) over quality approach. Reliable Data Transmission Challenge: Ensuring accurate data transmission from the wearable device to the rehabilitation program. Solution: Utilized robust signal processing and communication protocols with Arduino Duo and SPI. Overcoming Component Availability Challenge: 8-channel variation of microchip used for signal processing were sold out. Solution: Used two 6-channel microchips in a cascade configuration PCB Soldering Challenge: Soldering SMD components and verifying functionality. Solution: Conducted detailed analysis of PCB schematics and microchip datasheets; used a multimeter for circuit debugging to ensure proper connections and functionality of components.\n",
      "--------------------------------------------------\n",
      "Score: 0.06478893337698204\n",
      "Text: Component Integration: Challenge: Ensured reliable communication between sensors, controllers, and cameras. Solution: Thoroughly tested components, using high-quality connections and soldering techniques. Flight Stability: Challenge: Addressed issues like motor twitching and calibration errors. Solution: Analyzed flight logs, performed comprehensive calibrations, and adjusted hardware and software configurations. Autonomous Flight Implementation: Challenge: Faced complexity and safety concerns with computer vision-based autonomous flight. Solution: Conducted research and iterative development, integrating robust safety mechanisms.\n",
      "--------------------------------------------------\n",
      "Score: 0.06470588235294117\n",
      "Text: React Fundamentals: Initial difficulties with React's state management and component structure required revisiting and strengthening knowledge of React best practices, resulting in a more robust application. I spent several days reading the React Guide Frontend Design: I faced challenges in creating a cohesive and attractive UI. I am capable of imagining visually appealing designs but struggle to create them through CSS. I heavily relayed on ChatGPT to help bridge this gap through long discussions and iterative design processes. Time Management: The project extended beyond its initial timeline due to the complexity of the features and the iterative development process. Prioritizing the MVP allowed for timely completion of the essential features.\n",
      "--------------------------------------------------\n",
      "Score: 0.06280843765045739\n",
      "Text: Conceptualization: Extensive brainstorming sessions were conducted to define the project’s scope, objectives, and necessary technologies. This phase leveraged tools like ChatGPT to solidify ideas and create a project plan. Planning: Detailed project planning involved creating a a project outline, defining the project structure and outlining logic in state flow diagrams. Iterative Development: The project was built iteratively, starting with a basic mockup and gradually adding features while refactoring the codebase to improve organization and maintainability. Styling and UI Design: Special attention was given to designing an intuitive and visually appealing interface, which required several iterations and refinements. Testing and Debugging: The final stages of development focused on rigorous testing, bug fixing, and implementing error handling to ensure a stable and user-friendly product. Deployment: The project was deployed with a focus on creating a polished minimum viable product (MVP), prioritizing core features and postponing additional functionalities for future updates.\n",
      "--------------------------------------------------\n",
      "Score: 0.06162184606030949\n",
      "Text: Network Configuration: Challenge: Ensuring reliable and stable internet connectivity for both the Jetson Nano and Intel Processor presented several difficulties. The Jetson Nano, equipped with both Wi-Fi and Ethernet capabilities, had to be properly configured to share its internet connection with the Intel Processor, which only supported Ethernet. Solution: Connected the Jetson Nano to the network via Ethernet, providing it with stable internet access. For the Intel Processor, configured the Jetson Nano to act as a gateway, allowing the Intel Processor to access the internet through the Jetson Nano’s connection. Environment Setup: Challenge: Setting up a compatible Python and ROS environment on an older Ubuntu version. Solution: Verifying if this was the root cause to various problems encountered; followed detailed installation steps and created virtual environments to manage dependencies and package versions.\n",
      "--------------------------------------------------\n",
      "Score: 0.061137944914036\n",
      "Text: Challenge: Long Cache Building Times: The cache-building process for large models could take up to 63 hours , making the tool impractical for frequent use. Solution: By parallelizing the cache building process across 40 available CPU cores, the process the work could evenly be distributed across all cores, effectively reducing the build times by 40x. Challenge: Tracing Across Models: Connecting and tracing signals across multiple interconnected Simulink models presented difficulties due to missing metadata and naming inconsistencies. Solution: Implemented stubs (text file of key-value pairs correcting the inconsistencies) to map signals across mismatched inports/outports, ensuring that all critical links between models were traced correctly.\n",
      "--------------------------------------------------\n",
      "Score: 0.047162673392181595\n",
      "Text: As mentioned previously, the node design underwent several iteration before the final design was selected. Styling and designing are not strong assets of mine, so this required special attention, time, and the assistance of ChatGPT and external references to come up with a design that (I hope) is simplistic and concise. Below are the main iterations of the node designs undergone:\n",
      "--------------------------------------------------\n",
      "Score: 0.04608479342418343\n",
      "Text: GPU Cluster Issues: Faced issues with distributing the model across several GPUs, extensive debugging resulted in conjecture that the issue stemmed from a driver incompatibility problem. The solution involved using a single GPU at reduced precision to fit the model on the GPU. Poor Data Quality: Encountered discrepancies in the dataset, such as mismatched variable names and commands. Addressed these by flagging issues, selecting high-quality examples, and producing documentation with recommendations and examples for fixing them. Reproducibility: Ensured near-identical results by setting a seed for consistent \"randomness.\"\n",
      "--------------------------------------------------\n",
      "Score: 0.045028884159318945\n",
      "Text: Solution: Implemented a callback function that is called once window rendering is completed, ensuring all elements are fully loaded before the window is displayed to the user.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ret = retriever.retrieve(\"What challenges were encountered and how were they overcome\")\n",
    "for r in ret:\n",
    "    print(f\"Score: {r.score}\")\n",
    "    print(f\"Text: {r.node.text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the query pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever,\n",
    "    response_mode=ResponseMode.COMPACT_ACCUMULATE,\n",
    "    use_async = True,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    \n",
    "    chain=chain,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module d5314752-a2f8-4f5d-9ae7-e3fe787369a6 with input: \n",
      "input: What challenges were encountered and how were they overcome\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 8b2cebc1-326f-4e94-9c47-de42e853c7f0 with input: \n",
      "input: What challenges were encountered and how were they overcome\n",
      "\n",
      "\u001b[0mGenerated queries:\n",
      "1. Challenges faced in project management and strategies to overcome them\n",
      "2. Common obstacles in team collaboration and effective solutions\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.run(input=\"What challenges were encountered and how were they overcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "--------------------------------------------------\n",
      "Response 1: Several challenges were encountered across different projects, each addressed with specific solutions:\n",
      "\n",
      "1. **Data Workflow and Text Formatting**: The challenge of maintaining text formatting when exporting data from a text editor to a Word document was overcome by testing various Node modules and Python packages, ultimately selecting the most suitable ones for the project.\n",
      "\n",
      "2. **Non-Intrusive User Reminders**: To remind users to log their work without disrupting their focus, a fade-in animation for reminder windows was implemented, ensuring they appeared subtly in the bottom right corner while remaining visible.\n",
      "\n",
      "3. **Conditional Application Runtime**: Ensuring the application only runs during user-defined working hours was addressed by implementing a settings feature that allows for flexible scheduling.\n",
      "\n",
      "4. **Image Resizing and Indented Lists Formatting**: Issues with images losing their dimensions and nested bullet points not retaining their format when exported were resolved by modifying the Python library responsible for the export process.\n",
      "\n",
      "5. **Error Handling and Invalid Inputs**: To prevent invalid user inputs in settings, alerts were implemented to provide immediate feedback and stop the application from proceeding with invalid configurations.\n",
      "\n",
      "6. **Window Rendering**: The problem of windows loading before rendering was fixed to enhance user experience by ensuring complete displays before user interaction.\n",
      "\n",
      "7. **React Fundamentals**: Initial difficulties with React's state management were overcome by revisiting and strengthening knowledge of best practices, leading to a more robust application.\n",
      "\n",
      "8. **Front-End Design**: Challenges in creating a cohesive UI were tackled through extensive discussions and iterative design processes, utilizing external assistance.\n",
      "\n",
      "9. **Time Management**: The project timeline extended due to feature complexity, but prioritizing the minimum viable product (MVP) allowed for timely completion of essential features.\n",
      "\n",
      "10. **Device Design**: For a user-friendly EMG device, extensive research led to the creation of a 3D printed armband that balanced data quantity and usability.\n",
      "\n",
      "11. **Reliable Data Transmission**: Ensuring accurate data transmission from a wearable device was achieved through robust signal processing and communication protocols.\n",
      "\n",
      "12. **Component Availability**: The shortage of specific microchips was circumvented by using two alternative microchips in a cascade configuration.\n",
      "\n",
      "13. **PCB Soldering**: Challenges with soldering SMD components were addressed through detailed analysis and circuit debugging.\n",
      "\n",
      "14. **Visualization Complexity**: Presenting data in an understandable format was enhanced by implementing interactive visuals and filters in Power BI.\n",
      "\n",
      "15. **Efficiency in Troubleshooting**: Streamlining the data extraction and loading process involved creating a GUI for easy configuration, improving the overall efficiency of the dashboard setup.\n",
      "\n",
      "16. **User Adoption and Feedback**: To ensure the dashboard met user needs, continuous engagement with engineers for feedback and conducting training sessions facilitated effective adoption.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Response:\n",
    "{'-' * 50}\n",
    "{response}\n",
    "{'-' * 50}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "keyword_index = SimpleKeywordTableIndex(vector_index.docstore, storage_context=storage_context) # double check the use of docstore here (before, was node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example executions for the Vector Index and Keyword Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# define custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\n",
    "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "# keyword query engine\n",
    "keyword_query_engine = RetrieverQueryEngine(\n",
    "    retriever=keyword_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
