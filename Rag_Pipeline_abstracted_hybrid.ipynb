{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logging\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        If parent_properties provided, merge them with each page's properties.\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties with specific handling for Name, Description, and Tags.\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract properties from a page.\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content by:\n",
    "        1. Replacing multiple spaces with single space\n",
    "        2. Removing spaces before newlines\n",
    "        3. Removing spaces after newlines\n",
    "        4. Removing empty lines\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"Extract text content from rich text array and normalize it.\"\"\"\n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"Retrieve all child blocks of a given block with their nesting level.\"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content.\n",
    "        Sub-headers are treated as text content with line breaks.\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"Merge a group of bullets into a single line, with sub-bullets inline.\"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Process a single page and its nested databases.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"Process entire database and return structured data.\"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.vector_stores.qdrant.base:Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 5000.36it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 3310.42it/s]\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 9480.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "COLLECTION_NAME = \"Notion_vector_store\"\n",
    "\n",
    "model_kwargs = {\"model\": \"gpt-4o-mini\", \n",
    "                \"temperature\": 0,\n",
    "                \"api_key\": OPENAI_API_KEY,\n",
    "                \"system_prompt\": \"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "                Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "                \"\"\"                \n",
    "                }\n",
    "\n",
    "llm_openai = OpenAI(**model_kwargs)\n",
    "llm_cohere = Cohere(model=\"command-r-plus\", api_key=CO_API_KEY)\n",
    "\n",
    "embed_model_openai = OpenAIEmbedding(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "embed_model_cohere = CohereEmbedding(model=\"embed-english-v3.0\", api_key=CO_API_KEY)\n",
    "\n",
    "# set up the vector store\n",
    "client = QdrantClient(location=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "aclient = AsyncQdrantClient(location=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    enable_hybrid=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the database\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.readers.notion import NotionPageReader\n",
    "\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID)\n",
    "\n",
    "documents = [Document(text=record['content'], metadata=record['properties']) for record in processed_data]\n",
    "# documents = NotionPageReader(integration_token=NOTION_TOKEN).load_data(\n",
    "#     database_ids=[DATABASE_ID]\n",
    "# )\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index over vector database\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from llama_index.core.storage.index_store.simple_index_store import SimpleIndexStore\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    index_store=SimpleIndexStore(),\n",
    "    vector_store=vector_store\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    embed_model=embed_model_openai,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[sentence_splitter, embed_model_openai],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving nodes using: default retrieval\n",
      "Retrieved 3 nodes.\n",
      "\n",
      "\n",
      "Score: 0.56 - React Frontend (UI) React: The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow: A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS: Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks: Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js: Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap....\n",
      "-----\n",
      "\n",
      "Score: 0.55 - Asynchronous Operations: Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture: The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management: State management is distributed across both the Chrome extension and the React app....\n",
      "-----\n",
      "\n",
      "Score: 0.54 - Using context menus, mutation observers, and Chrome's messaging API, it reacts dynamically to changes in the browser's DOM to perform various actions, such as capturing selected text or opening new chats. React Aspect of the Project: The React portion of this project represents the user interface (UI) component of the browser extension, allowing users to interact with a graphical interface that organizes, displays, and manages nodes and conversations retrieved from ChatGPT. The extension’s React frontend allows users to navigate between different \"node spaces\" (essentially different workspaces or threads), visualize the relationships between nodes, and manage these nodes through actions like renaming, deleting, or creating new branches. The core functionality is built around React and React Flow , a library that facilitates building and managing node-based diagrams ....\n",
      "-----\n",
      "\n",
      "Retrieval with default complete...\n",
      "\n",
      "\n",
      "Retrieving nodes using: bm25 retrieval\n",
      "Retrieved 3 nodes.\n",
      "\n",
      "\n",
      "Score: 1.00 - React Frontend (UI) React: The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow: A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS: Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks: Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js: Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap....\n",
      "-----\n",
      "\n",
      "Score: 0.00 - Asynchronous Operations: Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture: The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management: State management is distributed across both the Chrome extension and the React app....\n",
      "-----\n",
      "\n",
      "Score: 0.00 - React Fundamentals: Initial difficulties with React's state management and component structure required revisiting and strengthening knowledge of React best practices, resulting in a more robust application. I spent several days reading the React Guide Frontend Design: I faced challenges in creating a cohesive and attractive UI. I am capable of imagining visually appealing designs but struggle to create them through CSS. I heavily relayed on ChatGPT to help bridge this gap through long discussions and iterative design processes. Time Management: The project extended beyond its initial timeline due to the complexity of the features and the iterative development process. Prioritizing the MVP allowed for timely completion of the essential features....\n",
      "-----\n",
      "\n",
      "Retrieval with bm25 complete...\n",
      "\n",
      "\n",
      "Retrieving nodes using: hybrid retrieval\n",
      "Retrieved 3 nodes.\n",
      "\n",
      "\n",
      "Score: 1.00 - React Frontend (UI) React: The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow: A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS: Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks: Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js: Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap....\n",
      "-----\n",
      "\n",
      "Score: 0.00 - Asynchronous Operations: Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture: The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management: State management is distributed across both the Chrome extension and the React app....\n",
      "-----\n",
      "\n",
      "Score: 0.00 - React Fundamentals: Initial difficulties with React's state management and component structure required revisiting and strengthening knowledge of React best practices, resulting in a more robust application. I spent several days reading the React Guide Frontend Design: I faced challenges in creating a cohesive and attractive UI. I am capable of imagining visually appealing designs but struggle to create them through CSS. I heavily relayed on ChatGPT to help bridge this gap through long discussions and iterative design processes. Time Management: The project extended beyond its initial timeline due to the complexity of the features and the iterative development process. Prioritizing the MVP allowed for timely completion of the essential features....\n",
      "-----\n",
      "\n",
      "Retrieval with hybrid complete...\n",
      "\n",
      "\n",
      "Retrieving nodes using: semantic_hybrid retrieval\n",
      "Retrieved 2 nodes.\n",
      "\n",
      "\n",
      "Score: 0.56 - React Frontend (UI) React: The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow: A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS: Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks: Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js: Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap....\n",
      "-----\n",
      "\n",
      "Score: 0.55 - Asynchronous Operations: Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture: The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management: State management is distributed across both the Chrome extension and the React app....\n",
      "-----\n",
      "\n",
      "Retrieval with semantic_hybrid complete...\n",
      "\n",
      "\n",
      "Retrieving nodes using: text_search retrieval\n",
      "Retrieved 3 nodes.\n",
      "\n",
      "\n",
      "Score: 0.56 - React Frontend (UI) React: The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow: A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS: Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks: Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js: Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap....\n",
      "-----\n",
      "\n",
      "Score: 0.55 - Asynchronous Operations: Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture: The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management: State management is distributed across both the Chrome extension and the React app....\n",
      "-----\n",
      "\n",
      "Score: 0.54 - Using context menus, mutation observers, and Chrome's messaging API, it reacts dynamically to changes in the browser's DOM to perform various actions, such as capturing selected text or opening new chats. React Aspect of the Project: The React portion of this project represents the user interface (UI) component of the browser extension, allowing users to interact with a graphical interface that organizes, displays, and manages nodes and conversations retrieved from ChatGPT. The extension’s React frontend allows users to navigate between different \"node spaces\" (essentially different workspaces or threads), visualize the relationships between nodes, and manage these nodes through actions like renaming, deleting, or creating new branches. The core functionality is built around React and React Flow , a library that facilitates building and managing node-based diagrams ....\n",
      "-----\n",
      "\n",
      "Retrieval with text_search complete...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QUERY_STRING = \"What projects involved the usage of React?\"\n",
    "\n",
    "def test_retrievers(query=QUERY_STRING, index=index, **kwargs):\n",
    "    retriever_engine = index.as_retriever(**kwargs)\n",
    "    retrieved_docs = retriever_engine.retrieve(query)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} nodes.\")\n",
    "    print(\"\\n\")\n",
    "    for node in retrieved_docs:\n",
    "        print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\\n\")\n",
    "    \n",
    "mode_kwargs = {\n",
    "    'default': {'vector_store_query_mode': 'default', 'similarity_top_k': 3},\n",
    "    'bm25': {'vector_store_query_mode':'hybrid', 'alpha': 0.0, 'hybrid_top_k': 3}, \n",
    "    'hybrid': {'vector_store_query_mode':'hybrid', 'alpha': 0.25, 'hybrid_top_k': 3},\n",
    "    'semantic_hybrid': {'vector_store_query_mode':'semantic_hybrid', 'alpha': 0.75, 'hybrid_top_k': 3},\n",
    "    # 'sparse': {\"sparse_top_k\":5},\n",
    "    'text_search': {'vector_store_query_mode':'text_search', 'similarity_top_k': 3},\n",
    "}\n",
    "\n",
    "for mode, kwargs in mode_kwargs.items():\n",
    "    print(f\"Retrieving nodes using: {mode} retrieval\")\n",
    "    test_retrievers(**kwargs)\n",
    "    print(f\"Retrieval with {mode} complete...\")        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core import PromptTemplate\n",
    "from prompts import QUESTION_GEN_PROMPT\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=1)\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE=PromptTemplate(QUESTION_GEN_PROMPT)\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=15,\n",
    "    num_queries=3,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=QUERY_GEN_PROMPT_TEMPLATE, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. Examples of leadership skills in the workplace\n",
      "2. Tips for showcasing leadership abilities in job interviews\n"
     ]
    }
   ],
   "source": [
    "nodes_with_scores = retriever.retrieve(\n",
    "    \"How did you demonstrate leadership?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.03 - Successfully created a minimum viable product (MVP) with essential logging functionalities. Implemented a user-friendly interface that allows seamless logging without disrupting the user's workflow. Integrated a robust reminder system with flexible settings for different user needs. Achieved a smooth user experience with optimized animations and transitions. Managed the entire project independently, from concept to final implementation, demonstrating strong project management and self-learning capabilities. Additional Development Insights: The project was conceptualized and developed independently, involving learning and applying new skills and technologies. Progressed through all software development phases, from initial concept to final implementation. Utilized GitHub Projects to keep track of tasks, maintain a work log, and ensure steady progress. Performed an initial assessment of technologies and selected Electron.js as the primary framework for its versatility and suitability for the project....\n",
      "-----\n",
      "\n",
      "Score: 0.02 - Designed, assembled, and flew a quadcopter in manual and semi-autonomous modes. Developed custom 3D-printed components. Successfully integrated and tested multiple sensors and communication modules with the quadcopter. Improved flight stability and performance through detailed calibration and tuning. Developed a functional communication setup between the quadcopter and ground control software. Conducted comprehensive flight tests and simulations to evaluate and optimize the system. Created a detailed documentation of the development and testing process, including logs and calibration notes. Enhanced practical skills in soldering, calibration, and flight testing....\n",
      "-----\n",
      "\n",
      "Score: 0.02 - Learn New Technologies: A primary focus was learning and applying React Software Skills Development: To gain experience in managing the full software development lifecycle, including frontend and backend development, debugging, and deployment. Challenge: Challenge myself to articulate, conceptualize, plan, and execute a project from start to finish within a reasonable timeframe, all of my own design. More information on how the project was managed on my thoughts throughout it can be found here ....\n",
      "-----\n",
      "\n",
      "Score: 0.02 - GPU Cluster Issues: Faced issues with distributing the model across several GPUs, extensive debugging resulted in conjecture that the issue stemmed from a driver incompatibility problem. The solution involved using a single GPU at reduced precision to fit the model on the GPU. Poor Data Quality: Encountered discrepancies in the dataset, such as mismatched variable names and commands. Addressed these by flagging issues, selecting high-quality examples, and producing documentation with recommendations and examples for fixing them. Reproducibility: Ensured near-identical results by setting a seed for consistent \"randomness.\"...\n",
      "-----\n",
      "\n",
      "Score: 0.02 - This project involved working with various OpenAI and open-source models to generate Software Unit Tests from Natural Language test case descriptions using Stellantis' test data. The primary focus was on evaluating and improving GPT-J with Stellantis' data using few-shot learning. Research into other generative strategies and training techniques was conducted in parallel. Project updates and detailed presentations were delivered to stakeholders to keep them informed of progress and findings. This project demonstrates advanced proficiency in NLP, showcasing the ability to handle complex data processing pipelines and large-scale datasets using state-of-the-art models and techniques. It incorporates best practices in software engineering, including modular design, error handling, and extensive logging, while also implementing performance optimization, experiment tracking, and reproducibility. The work explores cutting-edge techniques like prompt tuning and demonstrates versatility in data handling across various file systems and remote data sources, making it an excellent portfolio piece that highlights a comprehensive skill set in machine learning, software engineering, and data processing....\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HYPE_ANSWER_GEN_PROMPT' from 'prompts' (c:\\Users\\rbt7r\\OneDrive\\Documents\\VSCode Workspace\\RAG_Implementation\\prompts.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputComponent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryPipeline\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HYPE_ANSWER_GEN_PROMPT\n\u001b[0;32m      9\u001b[0m input_component \u001b[38;5;241m=\u001b[39m InputComponent()\n\u001b[0;32m     11\u001b[0m HYPE_ANSWER_GEN_PROMPT_TEMPLATE \u001b[38;5;241m=\u001b[39m PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'HYPE_ANSWER_GEN_PROMPT' from 'prompts' (c:\\Users\\rbt7r\\OneDrive\\Documents\\VSCode Workspace\\RAG_Implementation\\prompts.py)"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "rr_fusion_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever,\n",
    "    response_mode = ResponseMode.COMPACT_ACCUMULATE,\n",
    "    use_async = True,\n",
    "    text_qa_template = HYPE_ANSWER_GEN_PROMPT_TEMPLATE\n",
    "    )\n",
    "\n",
    "rr_fusion_chain = [input_component, rr_fusion_query_engine]\n",
    "\n",
    "rr_fusion_query_pipeline = QueryPipeline(\n",
    "    rr_fusion_chain,\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "\n",
    "query = \"Name projects and their associated challenges\"\n",
    "\n",
    "response = query_pipeline.run(input=query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically add more metadata to the documents\n",
    "\n",
    "from llama_index.core.extractors import KeywordExtractor, QuestionsAnsweredExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from prompts import KEYWORD_EXTRACT_PROMPT\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "keyword_extractor = KeywordExtractor(\n",
    "    keywords=5, \n",
    "    llm=llm_openai,\n",
    "    prompt_template=KEYWORD_EXTRACT_PROMPT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246/246 [00:32<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "transformations = [sentence_splitter, keyword_extractor, embed_model_openai]\n",
    "\n",
    "\n",
    "nodes = IngestionPipeline(\n",
    "    documents=documents,\n",
    "    transformations=transformations,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    embed_model=embed_model_openai,\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Excerpt from document]\n",
      "Employer: McMaster University - Partnered with Cubic Transportation\n",
      "Description: Deploying a Radar-Fusion Python script on Cubic’s GridSmart system, focusing on setting up the Python and ROS2 environment, interfacing external hardware, and ensuring stable network configuration between the Jetson Nano and Intel Processor\n",
      "Project Size: Small\n",
      "When: 2023-01-09\n",
      "Position: Research Engineer II\n",
      "Tags: Embedded System, Python, ROS2\n",
      "Name: Cubic Gridsmart (GS3) Deployment\n",
      "header: Tasks Performed:\n",
      "excerpt_keywords: Radar-Fusion, Jetson Nano, ROS2, Embedded Systems, Documentation\n",
      "Excerpt:\n",
      "-----\n",
      "ROS 2 Execution: Ran ROS 2 nodes and ensured proper communication between the hardware components and the GS3 system. Supervisory Role: Guidance and Support: Directed an undergraduate student, providing suggestions and assistance as needed. Documentation: Produced comprehensive documentation and user guides for setting up the GS3, accessing the Jetson Nano and Intel Processor, and configuring the ROS 2 and Python environments.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(nodes[50].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreInfo, MetadataInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"High level overviews of engineering projects from a work portfolio.\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"Name\",\n",
    "            type=\"str\",\n",
    "            description=\"Use this to identify the project name and create associations with other sections of the project.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"Tags\",\n",
    "            type=\"str\",\n",
    "            description=\"Tags associated with the overall project.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"excerpt_keywords\",\n",
    "            type=\"str\",\n",
    "            description=\"Keywords of specific skills demonstrated.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"Employer\",\n",
    "            type=\"str\",\n",
    "            description=\"Employer the project was done for.\"\n",
    "        ),\n",
    "        \n",
    "        MetadataInfo(\n",
    "            name=\"Position\",\n",
    "            type=\"str\",\n",
    "            description=\"Position the user held during the project.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"Description\",\n",
    "            type=\"str\",\n",
    "            description=\"Brief description of the project.\"\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"header\",\n",
    "            type=\"str\",\n",
    "            description=\"Header of the section.\"\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate with the Vector Index AutoRetriever\n",
    "\n",
    "from typing import List, Tuple, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AutoRetrieveModel(BaseModel):\n",
    "    query: str = Field(..., description=\"A question or statement on specific skills demonstrated in projects.\")\n",
    "    filter_key_list: List[str] = Field(\n",
    "        ..., description=\"List of metadata filter field names\"\n",
    "    )\n",
    "    filter_value_list: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of metadata filter field values (corresponding to names specified in filter_key_list)\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Retriever Functional Tool\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.vector_stores.types import MetadataFilter, FilterOperator, MetadataFilters\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "def auto_retrieve_fn(\n",
    "    query: str, filter_key_list: List[str], filter_value_list: List[str]):\n",
    "    \"\"\"Auto retrieval function.\n",
    "\n",
    "    Performs auto-retrieval from a vector database, and then applies a set of filters.\n",
    "\n",
    "    \"\"\"\n",
    "    query = query or \"Query\"\n",
    "\n",
    "    # Create a list of metadata filters by zipping together the filter keys and values\n",
    "    # Each filter checks if the metadata field contains the specified value\n",
    "    # For example, if filter_key_list=[\"Role\"] and filter_value_list=[\"Engineer\"]\n",
    "    # It will create a filter that checks if \"Engineer\" is contained in the \"Role\" metadata field\n",
    "    contains_filters = [\n",
    "        MetadataFilter(key=k, value=v, operator=FilterOperator.CONTAINS)\n",
    "        for k, v in zip(filter_key_list, filter_value_list)\n",
    "        ]\n",
    "\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index, \n",
    "        vector_store_query_mode=\"hybrid\",\n",
    "        alpha=0.65,\n",
    "        filters=MetadataFilters(filters=contains_filters),\n",
    "        top_k=top_k\n",
    "        )\n",
    "\n",
    "    query_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever,\n",
    "        response_mode=\"compact\",\n",
    "        verbose=True\n",
    "        )\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "description = f\"\"\"\\\n",
    "Use this tool to answer the user query by retrieving relevant information from the vector database.\n",
    "The vector database schema is given below, which you should use to find the right information to answer the user's query.:\n",
    "{vector_store_info.model_dump_json()}\n",
    "\"\"\"\n",
    "\n",
    "auto_retrieve_tool = FunctionTool.from_defaults(\n",
    "    fn=auto_retrieve_fn,\n",
    "    name=\"work-portfolio-info\",\n",
    "    description=description,\n",
    "    fn_schema=AutoRetrieveModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use this tool to answer the user query by retrieving relevant information from the vector database.\\nThe vector database schema is given below, which you should use to find the right information to answer the user\\'s query.:\\n{\"metadata_info\":[{\"name\":\"Name\",\"type\":\"str\",\"description\":\"Use this to identify the project name and create associations with other sections of the project.\"},{\"name\":\"Tags\",\"type\":\"str\",\"description\":\"Use this to identify the tags associated with the overall project.\"},{\"name\":\"excerpt_keywords\",\"type\":\"str\",\"description\":\"Use this to identify the keywords associated with specific sections of the project.\"},{\"name\":\"Employer\",\"type\":\"str\",\"description\":\"Use this when the user asks about the employer.\"},{\"name\":\"Position\",\"type\":\"str\",\"description\":\"Use this when the user asks about the job position.\"},{\"name\":\"Description\",\"type\":\"str\",\"description\":\"Use this to quickly identify relevant projects.\"}],\"content_info\":\"High level overviews of engineering projects from a work portfolio.\"}\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[auto_retrieve_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What projects involved the usage of React?\n",
      "=== Calling Function ===\n",
      "Calling function: work-portfolio-info with args: {\"query\":\"projects involving the usage of React\",\"filter_key_list\":[\"excerpt_keywords\"],\"filter_value_list\":[\"React\"]}\n",
      "Got output: The projects mentioned in the context involve the usage of React for software development.\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='The projects mentioned involve the usage of React for software development.', sources=[ToolOutput(content='The projects mentioned in the context involve the usage of React for software development.', tool_name='work-portfolio-info', raw_input={'args': (), 'kwargs': {'query': 'projects involving the usage of React', 'filter_key_list': ['excerpt_keywords'], 'filter_value_list': ['React']}}, raw_output='The projects mentioned in the context involve the usage of React for software development.', is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(\"What projects involved the usage of React?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest to Vector Database\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512, # in tokens\n",
    "    chunk_overlap=16, # in tokens\n",
    "    paragraph_separator=\"\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "tranforms = [\n",
    "    sentence_splitter, \n",
    "    embed_model_openai\n",
    "    ]\n",
    "\n",
    "nodes = IngestionPipeline(\n",
    "    documents=documents,\n",
    "    transformations=tranforms,\n",
    "    vector_store=vector_store\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_engine_kwargs = {\n",
    "    \"llm\": llm_openai,\n",
    "    \"embed_model\": embed_model_openai,\n",
    "    \"response_mode\": \"compact\",\n",
    "    \"similarity_top_k\": 15,\n",
    "    \"vector_store_query_mode\": \"mmr\",\n",
    "    \"vector_store_kwargs\": {\"mmr_threshold\": 0.42}\n",
    "    }\n",
    "\n",
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#     embed_model=embed_model_openai,\n",
    "#     vector_store=vector_store,\n",
    "#     )\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents=documents,\n",
    "#     embed_model=embed_model_openai,\n",
    "#     vector_store=vector_store\n",
    "#     )\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    documents,\n",
    "    show_progress=True,\n",
    "    store_nodes_override=True,\n",
    "    transformation=[sentence_splitter],\n",
    "    embed_model=embed_model_openai,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm_openai)\n",
    "#query_engine = index.as_chat_engine()\n",
    "#query_engine = index.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Pipeline\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "\n",
    "query_pipeline = QueryPipeline(\n",
    "    chain=chain,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "\n",
    "query = \"Name projects and their associated challenges\"\n",
    "\n",
    "response = query_pipeline.run(input=query)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
