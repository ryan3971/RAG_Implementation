{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM, Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbt7r\\OneDrive\\Documents\\VSCode Workspace\\RAG_Implementation\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"Notion_vector_store\"\n",
    "\n",
    "# Set up the LLM\n",
    "llm_openai = OpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.5,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "Settings.llm = llm_openai\n",
    "\n",
    "# Set up the Embeddings\n",
    "embed_model_openai = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "Settings.embed_model = embed_model_openai\n",
    "\n",
    "# set up the vector store client\n",
    "client = QdrantClient(\n",
    "    location=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the async client\n",
    "aclient = AsyncQdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up the Notion Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    \"\"\"\n",
    "    A class to process Notion databases and pages, extracting structured content and metadata.\n",
    "    Handles nested databases, headers, lists, and various text block types.\n",
    "    \"\"\"\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database with pagination support.\n",
    "        \n",
    "        Features:\n",
    "        - Handles pagination automatically using Notion's cursor-based system\n",
    "        - Optionally merges parent properties with each page's properties\n",
    "        - Processes all pages in database before returning\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): Notion database ID to query\n",
    "            parent_properties (Dict, optional): Properties to inherit from parent\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All pages in database with merged properties\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent database properties into individual page properties.\n",
    "        \n",
    "        Special handling for different property types:\n",
    "        - Name: Combines parent and child names with separator\n",
    "        - Description: Preserves child description over parent\n",
    "        - Tags: Merges parent and child tags, removing duplicates\n",
    "        - Other: Inherits parent property if not present in child\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Page object to update\n",
    "            parent_properties (Dict): Properties from parent database\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract and normalize page properties from Notion's API response.\n",
    "        \n",
    "        Handles various Notion property types:\n",
    "        - title: Page titles\n",
    "        - rich_text: Multi-line text\n",
    "        - select: Single select options\n",
    "        - multi_select: Multiple select options (converted to comma-separated string)\n",
    "        - date: Date fields (extracts start date)\n",
    "        - number/checkbox: Basic data types\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Raw Notion page object\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Normalized properties with consistent data types\n",
    "        \"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content for consistent formatting.\n",
    "        \n",
    "        Performs the following operations:\n",
    "        1. Replaces multiple spaces with single space\n",
    "        2. Removes spaces before colons\n",
    "        3. Strips whitespace from start/end of lines\n",
    "        4. Removes empty lines\n",
    "        5. Joins cleaned lines with newlines\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text content to normalize\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned and normalized text\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"\n",
    "        Extract plain text content from Notion's rich text format.\n",
    "        \n",
    "        Features:\n",
    "        - Combines multiple text segments\n",
    "        - Preserves plain text content\n",
    "        - Normalizes whitespace and formatting\n",
    "        \n",
    "        Args:\n",
    "            rich_text (List): Notion rich text array\n",
    "        \n",
    "        Returns:\n",
    "            str: Normalized plain text content\n",
    "        \"\"\"        \n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"\n",
    "        Recursively retrieve all child blocks of a given block.\n",
    "        \n",
    "        Features:\n",
    "        - Handles nested block structure\n",
    "        - Tracks nesting level for proper content organization\n",
    "        - Supports pagination for large block collections\n",
    "        - Skips recursion for child databases (handled separately)\n",
    "        \n",
    "        Args:\n",
    "            block_id (str): ID of block to get children for\n",
    "            level (int): Current nesting level (default: 0)\n",
    "        \n",
    "        Returns:\n",
    "            List[Tuple[Dict, int]]: List of (block, nesting_level) pairs\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract structured content organized by headers.\n",
    "        \n",
    "        Content organization:\n",
    "        - Level 1 headers start new sections\n",
    "        - Sub-headers are included in section content\n",
    "        - Bullet points are grouped and merged\n",
    "        - Paragraphs are added to current section\n",
    "        \n",
    "        Args:\n",
    "            blocks: List of (block, level) tuples to process\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Dict, List[str]]: \n",
    "                - Dict mapping headers to content section indices\n",
    "                - List of processed content sections\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"\n",
    "        Merge a group of bullet points into a single coherent text chunk.\n",
    "        \n",
    "        Handles nested bullet points by:\n",
    "        - Keeping main (level 0) bullets as separate lines\n",
    "        - Merging sub-bullets inline with their parent bullet\n",
    "        - Preserving the hierarchical relationship in the final text\n",
    "        \n",
    "        Args:\n",
    "            bullet_group: List of (text, level) tuples representing bullet hierarchy\n",
    "        \n",
    "        Returns:\n",
    "            str: Merged bullet points as normalized text\n",
    "        \"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page_whole(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page as a single document without splitting by headers.\n",
    "        \n",
    "        Features:\n",
    "        - Preserves headers as part of content\n",
    "        - Converts bullets to text with bullet markers\n",
    "        - Maintains paragraph structure\n",
    "        - Recursively processes child databases\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List containing single document with full page content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        content_parts = []\n",
    "        \n",
    "        for block, _ in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            if block_type.startswith('heading_'):\n",
    "                # Add headers as text with line breaks\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"{header_text}\\n\")\n",
    "                \n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                content_parts.append(f\"• {text_content}\")\n",
    "                \n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    content_parts.append(text_content)\n",
    "        \n",
    "        # Combine all content\n",
    "        full_content = self._normalize_text('\\n'.join(content_parts))\n",
    "        \n",
    "        if full_content:\n",
    "            results.append({\n",
    "                'properties': properties,\n",
    "                'content': full_content\n",
    "            })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_whole(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a page by splitting content at headers.\n",
    "        \n",
    "        Features:\n",
    "        - Creates separate chunks for each header section\n",
    "        - Preserves header hierarchy in properties\n",
    "        - Handles nested databases recursively\n",
    "        - Merges inherited properties appropriately\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page to process\n",
    "            parent_properties (Dict, optional): Properties to inherit\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of content chunks with associated properties\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_page_granular(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single page with granular text extraction for optimal chunking.\n",
    "        \n",
    "        Key features:\n",
    "        - Plain text blocks are merged into one chunk per header section\n",
    "        - List items are combined with their nested content into separate chunks\n",
    "        - Headers are preserved as metadata properties\n",
    "        - Handles nested databases recursively\n",
    "        \n",
    "        Args:\n",
    "            page (Dict): Notion page object to process\n",
    "            parent_properties (Dict, optional): Properties inherited from parent database\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed chunks with properties and content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        current_header = \"Main\"\n",
    "        current_text_chunk = []\n",
    "        current_list = []\n",
    "        in_list = False\n",
    "        \n",
    "        def save_text_chunk():\n",
    "            \"\"\"Helper to save accumulated text chunk\"\"\"\n",
    "            nonlocal current_text_chunk, results, properties, current_header\n",
    "            if current_text_chunk:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_text_chunk))\n",
    "                })\n",
    "                current_text_chunk = []\n",
    "        \n",
    "        def save_list_chunk():\n",
    "            \"\"\"Helper to save accumulated list chunk\"\"\"\n",
    "            nonlocal current_list, results, properties, current_header\n",
    "            if current_list:\n",
    "                chunk_properties = properties.copy()\n",
    "                chunk_properties['header'] = current_header\n",
    "                results.append({\n",
    "                    'properties': chunk_properties,\n",
    "                    'content': self._normalize_text('\\n'.join(current_list))\n",
    "                })\n",
    "                current_list = []\n",
    "        \n",
    "        prev_level = 0\n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                save_text_chunk()\n",
    "                save_list_chunk()\n",
    "                in_list = False\n",
    "                current_header = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "            # Handle list items\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                # If this is a new list (not in a list or level decreased)\n",
    "                if not in_list or level < prev_level:\n",
    "                    save_text_chunk()\n",
    "                    save_list_chunk()\n",
    "                    current_list.append(text_content)\n",
    "                    in_list = True\n",
    "                else:\n",
    "                    # Continue existing list\n",
    "                    current_list.append(text_content)\n",
    "                \n",
    "                prev_level = level\n",
    "                \n",
    "            # Handle paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    if in_list:\n",
    "                        # If we're in a list, append to the current list item\n",
    "                        current_list.append(text_content)\n",
    "                    else:\n",
    "                        # Otherwise, add to text chunk\n",
    "                        current_text_chunk.append(text_content)\n",
    "        \n",
    "        # Save any remaining chunks\n",
    "        save_text_chunk()\n",
    "        save_list_chunk()\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page_granular(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str, extraction_mode: str = 'header') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process entire database and return structured data.\n",
    "        \n",
    "        Args:\n",
    "            database_id (str): The ID of the Notion database to process\n",
    "            extraction_mode (str): Controls how content is extracted and chunked:\n",
    "                - 'header': splits pages by headers (default)\n",
    "                - 'whole': processes each page as a single document\n",
    "                - 'granular': extracts text blocks and list items separately\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of processed content chunks, each containing:\n",
    "                - properties: Dict of page metadata\n",
    "                - content: Extracted and normalized text content\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            if extraction_mode == 'header':\n",
    "                processed_data.extend(self.process_page(page))\n",
    "            elif extraction_mode == 'whole':\n",
    "                processed_data.extend(self.process_page_whole(page))\n",
    "            elif extraction_mode == 'granular':\n",
    "                processed_data.extend(self.process_page_granular(page))\n",
    "            else:\n",
    "                raise ValueError(\"extraction_mode must be one of: 'header', 'whole', 'granular'\")\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from the Notion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID, extraction_mode='granular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': 'GPT Chat Nav Pro Chrome Web Store'},\n",
       " {'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': 'The project is a Google Chrome extension designed to enhance the way users organize and interact with ChatGPT conversations by using a graphical node-based interface . The tool allows users to visualize conversations as nodes in a flow diagram, manage them through a side panel, and interact with them contextually via right-click menus. It leverages React for the frontend interface and Chrome APIs for background operations, storage, and messaging. The primary focus of the project is to improve productivity by allowing users to structure complex conversations, create branches from discussions, and easily navigate between chats in a visually intuitive manner.'},\n",
       " {'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': \"The chrome portion operates as a bridge between the user’s browser activity (primarily in ChatGPT) and a background script in the Chrome extension, maintaining the state of nodes and their associated data across different browser tabs. Using context menus, mutation observers, and Chrome's messaging API, it reacts dynamically to changes in the browser's DOM to perform various actions, such as capturing selected text or opening new chats.\"},\n",
       " {'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': 'The React portion of this project represents the user interface (UI) component of the browser extension, allowing users to interact with a graphical interface that organizes, displays, and manages nodes and conversations retrieved from ChatGPT. The extension’s React frontend allows users to navigate between different \"node spaces\" (essentially different workspaces or threads), visualize the relationships between nodes, and manage these nodes through actions like renaming, deleting, or creating new branches. The core functionality is built around React and React Flow , a library that facilitates building and managing node-based diagrams . The React app interacts closely with the Chrome Extension background script via message passing, and it uses various hooks, state management patterns, and event listeners to provide a responsive and dynamic user experience. More information on the software aspect of the project can be found here .'},\n",
       " {'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': 'Learn New Technologies: A primary focus was learning and applying React Software Skills Development: To gain experience in managing the full software development lifecycle, including frontend and backend development, debugging, and deployment. Challenge: Challenge myself to articulate, conceptualize, plan, and execute a project from start to finish within a reasonable timeframe, all of my own design. More information on how the project was managed on my thoughts throughout it can be found here .'},\n",
       " {'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool'},\n",
       "  'content': 'The Chrome extension is designed to enhance the usability and organization of ChatGPT conversations by introducing a structured approach to managing multiple chats. It enables users to create associations between chats, visualize the connections between them through a node-based diagram, and quickly navigate to specific chat conversation, making it ideal for research, brainstorming, and project management. More information on tool itself and its use can be found here'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[10:16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_metadata_keys = [\n",
    "                            #'Employer', \n",
    "                            #'Description', \n",
    "                            'Project Size',\n",
    "                            'When',\n",
    "                            'Position',\n",
    "                            'Tags', \n",
    "                            #'Name',\n",
    "                            #'header'\n",
    "                            ]\n",
    "\n",
    "documents = [Document(text=record['content'], \n",
    "                      metadata=record['properties'], \n",
    "                      excluded_embed_metadata_keys=excluded_metadata_keys, \n",
    "                      excluded_llm_metadata_keys=excluded_metadata_keys) \n",
    "            for record in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we need to:\n",
    "- Transform the data (Split large documents into chunks and produce Keywords that are also embedded)\n",
    "- Store in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "# Define the sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "#keyword_extractor = KeywordExtractor(keywords=5, llm=llm_openai)\n",
    "\n",
    "# define the transform\n",
    "transforms = [sentence_splitter, embed_model_openai]#, keyword_extractor]\n",
    "\n",
    "# Ingest into Database\n",
    "IngestionPipeline(\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
