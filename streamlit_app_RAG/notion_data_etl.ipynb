{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup and define Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment variables\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the LLM, Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"Notion_vector_store\"\n",
    "\n",
    "# Set up the LLM\n",
    "llm_openai = OpenAI(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    temperature=0.5,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "Settings.llm = llm_openai\n",
    "\n",
    "# Set up the Embeddings\n",
    "embed_model_openai = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-large\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "Settings.embed_model = embed_model_openai\n",
    "\n",
    "# set up the vector store client\n",
    "client = QdrantClient(\n",
    "    location=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the async client\n",
    "aclient = AsyncQdrantClient(\n",
    "    location=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    "    )\n",
    "# set up the vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    aclient=aclient, \n",
    "    collection_name=COLLECTION_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up the Notion Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        If parent_properties provided, merge them with each page's properties.\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties with specific handling for Name, Description, and Tags.\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract properties from a page.\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content by:\n",
    "        1. Replacing multiple spaces with single space\n",
    "        2. Removing spaces before newlines\n",
    "        3. Removing spaces after newlines\n",
    "        4. Removing empty lines\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"Extract text content from rich text array and normalize it.\"\"\"\n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"Retrieve all child blocks of a given block with their nesting level.\"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content.\n",
    "        Sub-headers are treated as text content with line breaks.\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"Merge a group of bullets into a single line, with sub-bullets inline.\"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Process a single page and its nested databases.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"Process entire database and return structured data.\"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from the Notion database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_metadata_keys = [\n",
    "                            #'Employer', \n",
    "                            #'Description', \n",
    "                            'Project Size',\n",
    "                            'When',\n",
    "                            'Position',\n",
    "                            'Tags', \n",
    "                            #'Name',\n",
    "                            #'header'\n",
    "                            ]\n",
    "\n",
    "documents = [Document(text=record['content'], \n",
    "                      metadata=record['properties'], \n",
    "                      excluded_embed_metadata_keys=excluded_metadata_keys, \n",
    "                      excluded_llm_metadata_keys=excluded_metadata_keys) \n",
    "            for record in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    documents[0].get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we need to:\n",
    "- Transform the data (Split large documents into chunks and produce Keywords that are also embedded)\n",
    "- Store in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "\n",
    "# Define the sentence splitter\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "#keyword_extractor = KeywordExtractor(keywords=5, llm=llm_openai)\n",
    "\n",
    "# define the transform\n",
    "transforms = [sentence_splitter, embed_model_openai]#, keyword_extractor]\n",
    "\n",
    "# Ingest into Database\n",
    "IngestionPipeline(\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    "    ).run(nodes=documents)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
