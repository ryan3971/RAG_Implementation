{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    \"\"\"A class to process and extract structured data from Notion databases and pages.\"\"\"\n",
    "    \n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        \n",
    "        Args:\n",
    "            database_id: The ID of the Notion database to query\n",
    "            parent_properties: Optional properties from parent pages to merge with results\n",
    "            \n",
    "        Returns:\n",
    "            List of page objects from the database\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties, avoiding duplicates.\n",
    "        \n",
    "        Args:\n",
    "            page: The page object to merge properties into\n",
    "            parent_properties: Properties from parent pages to merge\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key not in page['properties']:\n",
    "                # Create a new property of type 'rich_text' to store inherited values\n",
    "                page['properties'][f'parent_{key}'] = {\n",
    "                    'type': 'rich_text',\n",
    "                    'rich_text': [{\n",
    "                        'type': 'text',\n",
    "                        'text': {'content': str(value)},\n",
    "                        'plain_text': str(value)\n",
    "                    }]\n",
    "                }\n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract properties from a Notion page into a simplified dictionary format.\n",
    "        \n",
    "        Args:\n",
    "            page: The Notion page object\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of extracted properties\n",
    "        \"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                properties[prop_name] = [item['name'] for item in prop_data['multi_select']]\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"\n",
    "        Extract plain text content from Notion's rich text array format.\n",
    "        \n",
    "        Args:\n",
    "            rich_text: List of rich text objects from Notion\n",
    "            \n",
    "        Returns:\n",
    "            Concatenated plain text string\n",
    "        \"\"\"\n",
    "        return ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"\n",
    "        Retrieve all child blocks of a given block with their nesting level.\n",
    "        \n",
    "        Args:\n",
    "            block_id: ID of the parent block\n",
    "            level: Current nesting level (used for recursion)\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples containing (block, nesting_level)\n",
    "        \"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    # For child database blocks, don't process them here\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content sections.\n",
    "        \n",
    "        Args:\n",
    "            blocks: List of (block, level) tuples to process\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (headers_dict, content_sections) where headers_dict maps\n",
    "            headers to content section indices\n",
    "        \"\"\"\n",
    "        current_main_header = None\n",
    "        current_sub_headers = []\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_main_header, current_sub_headers, headers\n",
    "            \n",
    "            # Save any remaining bullet group\n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            # Save content if exists and we have a header\n",
    "            if current_main_header is not None and current_content:\n",
    "                content_sections.append('\\n'.join(filter(None, current_content)))\n",
    "                full_header = current_main_header\n",
    "                if current_sub_headers:\n",
    "                    full_header += ' - ' + ' - '.join(current_sub_headers)\n",
    "                headers[full_header] = len(content_sections) - 1  # Point to the section we just added\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                # Save current section before starting new one\n",
    "                save_current_section()\n",
    "                \n",
    "                # Reset content collection for new section\n",
    "                current_content = []\n",
    "                current_bullet_group = []\n",
    "                \n",
    "                # Process new header\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    current_main_header = header_text\n",
    "                    current_sub_headers = []\n",
    "                else:\n",
    "                    current_sub_headers.append(header_text)\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"\n",
    "        Merge a group of bullets into a single line, with sub-bullets inline.\n",
    "        \n",
    "        Args:\n",
    "            bullet_group: List of (text, level) tuples representing bullet points\n",
    "            \n",
    "        Returns:\n",
    "            Merged bullet points as a single string\n",
    "        \"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(' '.join(current_main_bullet))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(' '.join(current_main_bullet))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single page and its nested databases.\n",
    "        \n",
    "        Args:\n",
    "            page: The Notion page object to process\n",
    "            parent_properties: Optional properties from parent pages\n",
    "            \n",
    "        Returns:\n",
    "            List of processed entries with properties and content\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge parent properties if they exist\n",
    "        if parent_properties:\n",
    "            properties.update({f'parent_{k}': v for k, v in parent_properties.items()})\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Add headers to properties and create entries\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            # Create entry for this section\n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process any child databases found in the blocks\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process entire database and return structured data.\n",
    "        \n",
    "        Args:\n",
    "            database_id: The ID of the Notion database to process\n",
    "            \n",
    "        Returns:\n",
    "            List of processed entries with properties and content\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        # Get all pages from the database\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        # Process each page and its nested databases\n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data\n",
    "\n",
    "# Example usage in Jupyter notebook (separate cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from notion_client import Client\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class NotionProcessor:\n",
    "    def __init__(self, auth_token: str):\n",
    "        \"\"\"Initialize the Notion client with authentication token.\"\"\"\n",
    "        self.notion = Client(auth=auth_token)\n",
    "    \n",
    "    def get_database_pages(self, database_id: str, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve all pages from a Notion database.\n",
    "        If parent_properties provided, merge them with each page's properties.\n",
    "        \"\"\"\n",
    "        pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.databases.query(\n",
    "                database_id=database_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            # If parent properties exist, merge them with each page\n",
    "            if parent_properties:\n",
    "                for page in response['results']:\n",
    "                    self._merge_parent_properties(page, parent_properties)\n",
    "            \n",
    "            pages.extend(response['results'])\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return pages\n",
    "    \n",
    "    def _merge_parent_properties(self, page: Dict, parent_properties: Dict):\n",
    "        \"\"\"\n",
    "        Merge parent properties into page properties with specific handling for Name, Description, and Tags.\n",
    "        \"\"\"\n",
    "        for key, value in parent_properties.items():\n",
    "            if key == 'Name':\n",
    "                # Handle name merging only if child page has a name\n",
    "                if 'Name' in page['properties']:\n",
    "                    child_name = self._get_rich_text_content(page['properties']['Name'].get('title', []))\n",
    "                    if child_name:\n",
    "                        merged_name = f\"{value} - {child_name}\"\n",
    "                        page['properties']['Name'] = {\n",
    "                            'type': 'title',\n",
    "                            'title': [{\n",
    "                                'type': 'text',\n",
    "                                'text': {'content': merged_name},\n",
    "                                'plain_text': merged_name\n",
    "                            }]\n",
    "                        }\n",
    "            elif key == 'Description':\n",
    "                # Skip Description property - keep child's description if it exists\n",
    "                continue\n",
    "            elif key == 'Tags':\n",
    "                # Merge tags, removing duplicates\n",
    "                parent_tags = set(value.split(', ')) if value else set()\n",
    "                if 'Tags' in page['properties']:\n",
    "                    child_tags = set(tag['name'] for tag in page['properties']['Tags'].get('multi_select', []))\n",
    "                    merged_tags = parent_tags.union(child_tags)\n",
    "                    page['properties']['Tags'] = {\n",
    "                        'type': 'multi_select',\n",
    "                        'multi_select': [{'name': tag} for tag in sorted(merged_tags)]\n",
    "                    }\n",
    "            else:\n",
    "                # For all other properties, inherit from parent if not present in child\n",
    "                if key not in page['properties']:\n",
    "                    page['properties'][key] = {\n",
    "                        'type': 'rich_text',\n",
    "                        'rich_text': [{\n",
    "                            'type': 'text',\n",
    "                            'text': {'content': str(value)},\n",
    "                            'plain_text': str(value)\n",
    "                        }]\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def extract_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"Extract properties from a page.\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        for prop_name, prop_data in page['properties'].items():\n",
    "            prop_type = prop_data['type']\n",
    "            \n",
    "            if prop_type == 'title':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['title'])\n",
    "            elif prop_type == 'rich_text':\n",
    "                properties[prop_name] = self._get_rich_text_content(prop_data['rich_text'])\n",
    "            elif prop_type == 'select':\n",
    "                if prop_data['select']:\n",
    "                    properties[prop_name] = prop_data['select']['name']\n",
    "            elif prop_type == 'multi_select':\n",
    "                # Convert multi-select to comma-separated string\n",
    "                properties[prop_name] = ', '.join(sorted(item['name'] for item in prop_data['multi_select']))\n",
    "            elif prop_type == 'date':\n",
    "                if prop_data['date']:\n",
    "                    properties[prop_name] = prop_data['date']['start']\n",
    "            elif prop_type in ['number', 'checkbox']:\n",
    "                properties[prop_name] = prop_data[prop_type]\n",
    "                \n",
    "        return properties\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text content by:\n",
    "        1. Replacing multiple spaces with single space\n",
    "        2. Removing spaces before newlines\n",
    "        3. Removing spaces after newlines\n",
    "        4. Removing empty lines\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove spaces before colons\n",
    "        text = text.replace(' :', ':')\n",
    "        \n",
    "        # Split into lines and process each line\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Clean each line individually\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Only keep non-empty lines\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def _get_rich_text_content(self, rich_text: List) -> str:\n",
    "        \"\"\"Extract text content from rich text array and normalize it.\"\"\"\n",
    "        text = ' '.join([text['plain_text'] for text in rich_text if text.get('plain_text')])\n",
    "        return self._normalize_text(text)\n",
    "    \n",
    "    def get_block_children(self, block_id: str, level: int = 0) -> List[Tuple[Dict, int]]:\n",
    "        \"\"\"Retrieve all child blocks of a given block with their nesting level.\"\"\"\n",
    "        blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.notion.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            \n",
    "            for block in response['results']:\n",
    "                blocks.append((block, level))\n",
    "                \n",
    "                if block.get('has_children'):\n",
    "                    if block['type'] != 'child_database':\n",
    "                        child_blocks = self.get_block_children(block['id'], level + 1)\n",
    "                        blocks.extend(child_blocks)\n",
    "            \n",
    "            if not response.get('has_more'):\n",
    "                break\n",
    "                \n",
    "            cursor = response['next_cursor']\n",
    "            \n",
    "        return blocks\n",
    "    \n",
    "    def process_blocks(self, blocks: List[Tuple[Dict, int]]) -> Tuple[Dict, List[str]]:\n",
    "        \"\"\"\n",
    "        Process blocks to extract headers and content.\n",
    "        Sub-headers are treated as text content with line breaks.\n",
    "        \"\"\"\n",
    "        current_header = None\n",
    "        current_content = []\n",
    "        headers = {}\n",
    "        content_sections = []\n",
    "        current_bullet_group = []\n",
    "        \n",
    "        def save_current_section():\n",
    "            \"\"\"Helper function to save current section's content.\"\"\"\n",
    "            nonlocal current_content, current_bullet_group, content_sections, current_header, headers\n",
    "            \n",
    "            if current_bullet_group:\n",
    "                current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                current_bullet_group = []\n",
    "            \n",
    "            if current_header is not None and current_content:\n",
    "                # Join content and normalize the entire section\n",
    "                section_content = self._normalize_text('\\n'.join(filter(None, current_content)))\n",
    "                content_sections.append(section_content)\n",
    "                headers[current_header] = len(content_sections) - 1\n",
    "        \n",
    "        for block, level in blocks:\n",
    "            block_type = block['type']\n",
    "            \n",
    "            # Handle headers\n",
    "            if block_type.startswith('heading_'):\n",
    "                header_text = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                header_level = int(block_type[-1])\n",
    "                \n",
    "                if header_level == 1:\n",
    "                    # Save current section before starting new one\n",
    "                    save_current_section()\n",
    "                    current_content = []\n",
    "                    current_header = header_text\n",
    "                else:\n",
    "                    # Treat sub-headers as text content with line break\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_content.append(f\"{header_text}\\n\")\n",
    "            \n",
    "            # Handle child database\n",
    "            elif block_type == 'child_database':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                current_content.append(f\"[Database: {block['id']}]\")\n",
    "            \n",
    "            # Handle bullet points and numbered lists\n",
    "            elif block_type in ['bulleted_list_item', 'numbered_list_item']:\n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                \n",
    "                if level == 0:\n",
    "                    if current_bullet_group:\n",
    "                        current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                        current_bullet_group = []\n",
    "                    current_bullet_group = [(text_content, level)]\n",
    "                else:\n",
    "                    current_bullet_group.append((text_content, level))\n",
    "            \n",
    "            # Handle regular paragraphs\n",
    "            elif block_type == 'paragraph':\n",
    "                if current_bullet_group:\n",
    "                    current_content.append(self._merge_bullet_group(current_bullet_group))\n",
    "                    current_bullet_group = []\n",
    "                \n",
    "                text_content = self._get_rich_text_content(block[block_type]['rich_text'])\n",
    "                if text_content:\n",
    "                    current_content.append(text_content)\n",
    "        \n",
    "        # Save final section\n",
    "        save_current_section()\n",
    "        \n",
    "        return headers, content_sections\n",
    "    \n",
    "    def _merge_bullet_group(self, bullet_group: List[Tuple[str, int]]) -> str:\n",
    "        \"\"\"Merge a group of bullets into a single line, with sub-bullets inline.\"\"\"\n",
    "        if not bullet_group:\n",
    "            return \"\"\n",
    "        \n",
    "        result = []\n",
    "        current_main_bullet = []\n",
    "        \n",
    "        for text, level in bullet_group:\n",
    "            if level == 0:\n",
    "                if current_main_bullet:\n",
    "                    result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "                current_main_bullet = [text]\n",
    "            else:\n",
    "                current_main_bullet.append(text)\n",
    "        \n",
    "        if current_main_bullet:\n",
    "            result.append(self._normalize_text(' '.join(current_main_bullet)))\n",
    "        \n",
    "        return '\\n'.join(result)\n",
    "    \n",
    "    def process_page(self, page: Dict, parent_properties: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Process a single page and its nested databases.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Extract properties before any merging\n",
    "        properties = self.extract_properties(page)\n",
    "        \n",
    "        # Merge with parent properties if they exist\n",
    "        if parent_properties:\n",
    "            # Skip special properties handling here as it's done in _merge_parent_properties\n",
    "            # Only handle properties that weren't merged during database query\n",
    "            for key, value in parent_properties.items():\n",
    "                if key not in ['Name', 'Description', 'Tags'] and key not in properties:\n",
    "                    properties[key] = value\n",
    "        \n",
    "        # Process page blocks\n",
    "        blocks = self.get_block_children(page['id'])\n",
    "        headers, content_sections = self.process_blocks(blocks)\n",
    "        \n",
    "        # Create entries for each section\n",
    "        for header, section_index in headers.items():\n",
    "            section_properties = properties.copy()\n",
    "            section_properties['header'] = header\n",
    "            \n",
    "            if 0 <= section_index < len(content_sections):\n",
    "                results.append({\n",
    "                    'properties': section_properties,\n",
    "                    'content': content_sections[section_index]\n",
    "                })\n",
    "        \n",
    "        # Process child databases\n",
    "        for block, _ in blocks:\n",
    "            if block['type'] == 'child_database':\n",
    "                child_pages = self.get_database_pages(block['id'], properties)\n",
    "                for child_page in child_pages:\n",
    "                    results.extend(self.process_page(child_page, properties))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def process_database(self, database_id: str) -> List[Dict]:\n",
    "        \"\"\"Process entire database and return structured data.\"\"\"\n",
    "        processed_data = []\n",
    "        pages = self.get_database_pages(database_id)\n",
    "        \n",
    "        for page in pages:\n",
    "            processed_data.extend(self.process_page(page))\n",
    "            \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in Jupyter notebook (separate cells):\n",
    "\n",
    "# Cell 1: Initialize processor\n",
    "NOTION_TOKEN = os.getenv(\"NOTION_TOKEN\")\n",
    "DATABASE_ID = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "\n",
    "processor = NotionProcessor(NOTION_TOKEN)\n",
    "processed_data = processor.process_database(DATABASE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'properties': {'Employer': 'Personal',\n",
       "   'Description': 'A Chrome Extension for Structured ChatGPT Chat Management and Navigation',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, React, Web Development',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool',\n",
       "   'header': 'Tool Purpose:'},\n",
       "  'content': 'The Chrome extension is designed to enhance the usability and organization of ChatGPT conversations by introducing a structured approach to managing multiple chats. It enables users to create associations between chats, visualize the connections between them through a node-based diagram, and quickly navigate to specific chat conversation, making it ideal for research, brainstorming, and project management. More information on tool itself and its use can be found here [Database: 906e9719-ccb1-4ddd-b800-abda20f123bf]'},\n",
       " {'properties': {'Tags': 'Boostrap, CSS, ChatGPT, Chrome, Chrome Extension, HTML, JavaScript, React, React-Flow, Tailwind, VS Code, Vite, Web Development, npm',\n",
       "   'Description': 'A high-level overview focusing on the software development of this project',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Software Development Overview',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'Core Functionality'},\n",
       "  'content': 'The extension consists of two main components: the Chrome background script and the React frontend . Node-Based ChatGPT Conversation Management : The core feature of the tool is its ability to represent ChatGPT conversations as nodes in a flow diagram . Each node represents a conversation, while the edges between nodes represent relationships or continuations from parent to child nodes. Users can visually organize these conversations, making it easier to track, associate and navigate conversations. Node Spaces : The concept of node spaces acts as different workspaces or projects. Each node space contains its own set of nodes, all generally falling under a common theme or topic. Users can switch between node spaces, rename them, or delete them from a side panel within the UI. Branching Conversations : One of the most powerful features of the tool is the ability to create branches from a conversation. Users can select text from a chat and branch off a new conversation, visually representing the relationship between the main conversation and its sub-discussions. Context Menus and Node Management : Right-click context menus provide quick access to actions like renaming, deleting, or opening nodes and their associated chats. This ensures users can easily manage their nodes and spaces directly from the visual interface without needing to switch to another panel or view. Persistent Data Storage : The tool uses Chrome’s Storage API to persistently save node spaces, node titles, messages, and branches. This ensures that all conversation data is retained even after browser restarts or closing the extension, allowing users to pick up where they left off. Automatic Layout and Visualization : The tool integrates ELK.js to automatically layout nodes in a hierarchical structure. This ensures that nodes are spaced evenly, the diagram fully gits into view, and parent-child relationships are visually clear and organized, improving the overall user experience.'},\n",
       " {'properties': {'Tags': 'Boostrap, CSS, ChatGPT, Chrome, Chrome Extension, HTML, JavaScript, React, React-Flow, Tailwind, VS Code, Vite, Web Development, npm',\n",
       "   'Description': 'A high-level overview focusing on the software development of this project',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Software Development Overview',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'Technologies Used'},\n",
       "  'content': 'Chrome Extension (Backend) Chrome Storage API : Used to store conversation data, node spaces, and related information persistently. Chrome Messaging API : Enables communication between the background script , content scripts , and the React frontend . The messaging API allows for the dynamic transfer of data, including node creation, deletion, and updates. Background Script : Handles the core logic for creating new nodes, managing conversations, and responding to context menu events. It also coordinates state management across multiple tabs. Content Script : Embedded into the ChatGPT webpage, the content script monitors for changes, such as new messages or conversations, and interacts with the background script to ensure conversation data is updated in real time. React Frontend (UI) React : The primary framework used to build the user interface. It allows for component-based development, ensuring reusability and scalability. React Flow : A specialized library for visualizing nodes and edges, which is central to displaying the conversation flow and relationships between nodes. Bootstrap & Tailwind CSS : Used for styling the components and ensuring a clean, responsive UI. Bootstrap provides pre-built elements like buttons and panels, while Tailwind ensures utility-first, custom styling. Custom Hooks : Utilized to manage state, handle events, and communicate with Chrome storage and the background script efficiently. ELK.js : Provides automatic node layout functionality, ensuring that nodes are arranged hierarchically and without overlap. Toast Notifications : Used to alert users to the success or failure of actions, providing real-time feedback in a non-intrusive manner.'},\n",
       " {'properties': {'Tags': 'Boostrap, CSS, ChatGPT, Chrome, Chrome Extension, HTML, JavaScript, React, React-Flow, Tailwind, VS Code, Vite, Web Development, npm',\n",
       "   'Description': 'A high-level overview focusing on the software development of this project',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Software Development Overview',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'Software Techniques and Best Practices'},\n",
       "  'content': 'Asynchronous Operations : Both the Chrome background script and the React app rely heavily on async/await to handle data fetching, storage operations, and message passing. This ensures that the UI remains responsive even during long-running operations. Event-Driven Architecture : The project employs an event-driven architecture through Chrome’s messaging system, where user interactions (e.g., right-clicking a node, selecting a workspace) trigger events that are handled by the background script or React app. State Management : State management is distributed across both the Chrome extension and the React app. The background script manages the core logic and persistent state (e.g., node spaces), while the React app manages the local UI state (e.g., whether the side panel is open, or which node is selected). Custom Node Components : The project uses custom React Flow nodes to represent different types of conversations (e.g., parent nodes and branch nodes). These nodes can display messages, provide buttons for user actions, and respond to events like dragging or selection. Contextual Operations with Context Menus : Right-click context menus are used to provide quick, contextual actions. These menus allow users to efficiently manage nodes (e.g., rename, delete) without leaving the flow diagram, improving usability.'},\n",
       " {'properties': {'Tags': 'Boostrap, CSS, ChatGPT, Chrome, Chrome Extension, HTML, JavaScript, React, React-Flow, Tailwind, VS Code, Vite, Web Development, npm',\n",
       "   'Description': 'A high-level overview focusing on the software development of this project',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Software Development Overview',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'High-Level Workflow'},\n",
       "  'content': 'Conversation Monitoring : The content script monitors the ChatGPT interface for new messages, updates to conversation threads, and other changes. When a significant event (e.g., a new message or conversation) is detected, it communicates with the background script to update the stored data. Background Script Operations : The background script manages the state of the nodes and conversations across all tabs. It handles tasks like creating new nodes, updating node messages, and renaming nodes. It also communicates with the React frontend to synchronize the visual state. React Flow Diagram : The React frontend renders a flow diagram using React Flow , allowing users to see and interact with their conversations in a node-based layout. Users can perform actions like dragging nodes, creating branches, and visualizing relationships between conversations. Node Spaces and Side Panel : Users can manage multiple node spaces via the side panel . The panel lists all available spaces, and users can switch between them or create new ones. Each space contains its own set of nodes and conversations.'},\n",
       " {'properties': {'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, Project Management, React, State Flow Diagram, Web Development',\n",
       "   'Description': 'A detailed overview of how this project was managed',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Project Management and Notes',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'Tasks Performed:'},\n",
       "  'content': 'Conceptualization : Extensive brainstorming sessions were conducted to define the project’s scope, objectives, and necessary technologies. This phase leveraged tools like ChatGPT to solidify ideas and create a project plan. Planning : Detailed project planning involved creating a a project outline, defining the project structure and outlining logic in state flow diagrams. Iterative Development : The project was built iteratively, starting with a basic mockup and gradually adding features while refactoring the codebase to improve organization and maintainability. Styling and UI Design : Special attention was given to designing an intuitive and visually appealing interface, which required several iterations and refinements. Testing and Debugging : The final stages of development focused on rigorous testing, bug fixing, and implementing error handling to ensure a stable and user-friendly product. Deployment : The project was deployed with a focus on creating a polished minimum viable product (MVP), prioritizing core features and postponing additional functionalities for future updates.'},\n",
       " {'properties': {'Tags': 'CSS, ChatGPT, Chrome Extension, HTML, JavaScript, Project Management, React, State Flow Diagram, Web Development',\n",
       "   'Description': 'A detailed overview of how this project was managed',\n",
       "   'Name': 'ChatGPT Organization and Navigation Tool - Project Management and Notes',\n",
       "   'Employer': 'Personal',\n",
       "   'Project Size': 'Large',\n",
       "   'When': '2024-07-08',\n",
       "   'Position': 'Fun',\n",
       "   'header': 'Challenges and Solutions:'},\n",
       "  'content': \"React Fundamentals : Initial difficulties with React's state management and component structure required revisiting and strengthening knowledge of React best practices, resulting in a more robust application. I spent several days reading the React Guide Frontend Design : I faced challenges in creating a cohesive and attractive UI. I am capable of imagining visually appealing designs but struggle to create them through CSS. I heavily relayed on ChatGPT to help bridge this gap through long discussions and iterative design processes. Time Management : The project extended beyond its initial timeline due to the complexity of the features and the iterative development process. Prioritizing the MVP allowed for timely completion of the essential features.\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[8:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = [Document(text=record['content'], metadata=record['properties']) for record in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '05d66872-f820-4560-9ce3-a2a8b463c08a',\n",
       " 'embedding': None,\n",
       " 'metadata': {'Employer': 'McMaster University - Partnered with Stellantis',\n",
       "  'Description': 'Maintained and improved the  Boundary Diagram Tool  (BDT) for change impact analysis in large-scale Simulink models, specifically for automotive systems at Stellantis   ',\n",
       "  'Project Size': 'Medium',\n",
       "  'When': '2020-06-01',\n",
       "  'Position': 'Research Assistant',\n",
       "  'Tags': ['MATLAB', 'Simulink'],\n",
       "  'Name': 'MATLAB/Simulink - Boundary Diagram Tool',\n",
       "  'header': 'Project Overview:'},\n",
       " 'excluded_embed_metadata_keys': [],\n",
       " 'excluded_llm_metadata_keys': [],\n",
       " 'relationships': {},\n",
       " 'text': 'The  Boundary Diagram Tool (BDT)  is an advanced tool designed for  change impact analysis  in large-scale Simulink models, particularly those used in embedded systems like automotive control units. The tool enables engineers to trace how changes in specific parts of a system propagate through other models and network interfaces, aiding in software maintenance, debugging, and ensuring compliance with safety standards like  ISO 26262 . It was developed as part of a collaboration with  Stellantis  (formerly FCA), where it played a key role in managing changes within complex automotive systems, such as  hybrid electric vehicle control systems .\\nAs part of the development team, my work on the BDT Tool involved both using the tool for various critical tasks and improving its functionality to make it more efficient and reliable.',\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 96/96 [00:00<00:00, 2128.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512, # in tokens\n",
    "    chunk_overlap=16, # in tokens\n",
    "    paragraph_separator=\"\\n\\n\\n\"\n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(documents, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")\n",
    "\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "QDRANT_URL = os.environ['QDRANT_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "# initialize qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"Notion_vector_store\",\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "# assign qdrant vector store to storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 99/99 [00:01<00:00, 52.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/index?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/index?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/index?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import  VectorStoreIndex\n",
    "\n",
    "# create the index\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    show_progress=True,\n",
    "    store_nodes_override=True,\n",
    "    #transformation=[sentence_splitter],\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "# configure a retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=15,\n",
    ")\n",
    "\n",
    "# configure a post processor\n",
    "similarity_processor = SimilarityPostprocessor(similarity_cutoff=0.3)\n",
    "\n",
    "# configure a response sythesizer\n",
    "response_synthsizer = get_response_synthesizer(llm=llm)\n",
    "\n",
    "# create a query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthsizer,\n",
    "    node_postprocessors=[similarity_processor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://0a690c49-5e88-4998-82fe-3be37b2cc61d.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Notion_vector_store/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.com/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.com/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Outline work done that involved the use of embedded systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5184556\n",
      "0.5076841\n",
      "0.5056155\n",
      "0.504579\n",
      "0.5045206\n",
      "0.485343\n",
      "0.47040236\n",
      "0.46849632\n",
      "0.4519909\n",
      "0.4500804\n",
      "0.44908825\n",
      "0.44807273\n",
      "0.44527188\n",
      "0.4442506\n",
      "0.4425006\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(node.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been involved in several projects that utilized embedded systems:\n",
      "\n",
      "- Cobra 55 Bootloader RAM Project: Developed a custom bootloader for the Cobra 55 microcontroller, allowing it to establish an Ethernet connection, receive a program, store it in RAM, and execute it. This project involved in-depth knowledge of embedded systems memory management and bootloader development.\n",
      "\n",
      "- Stroke Rehabilitation Using EMG-Driven Tetris Game: Designed and built a wearable device for stroke patients to rehabilitate fine-motor skills. This involved using surface EMG technology to collect data, process it using embedded software, and transmit it to a Tetris game for rehabilitation.\n",
      "\n",
      "- Quadcopter Development and Testing: Assembled, tested, and programmed a quadcopter, including working with various embedded systems such as the Pixhawk flight controller, sensors, and the Raspberry Pi for enhanced processing and computer vision capabilities.\n",
      "\n",
      "- Cubic Gridsmart (GS3) Deployment: Deployed a Radar-Fusion Python script on Cubic's GridSmart system, which involved setting up a Python and ROS2 environment, interfacing with external hardware, and ensuring stable network configuration between a Jetson Nano and an Intel Processor.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
